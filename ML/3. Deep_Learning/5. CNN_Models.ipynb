{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Models - LeNet5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fashion mnist - 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1) (60000, 10)\n",
      "(10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 데이터 준비\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 28, 28, 1)\n",
    "x_test = x_test.reshape(10000,28,28,1)\n",
    "\n",
    "# 텐서플로우의 원핫 인코딩 함수 ( 또는 판다스의 get_dummies)\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 24, 24, 3)         78        \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 12, 12, 3)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 432)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 84)                36372     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37,300\n",
      "Trainable params: 37,300\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "X = tf.keras.Input(shape=[28,28,1])\n",
    "H = tf.keras.layers.Conv2D(3,5,activation=\"swish\") ( X)\n",
    "H = tf.keras.layers.MaxPool2D()(H)\n",
    "H = tf.keras.layers.Conv2D(3,5,activation=\"swish\") ( X)\n",
    "H = tf.keras.layers.MaxPool2D()(H)\n",
    "H = tf.keras.layers.Flatten() (H)\n",
    "H = tf.keras.layers.Dense(84, activation=\"swish\")(H)\n",
    "Y = tf.keras.layers.Dense(10,activation=\"softmax\") (H)\n",
    "\n",
    "model = tf.keras.Model(X,Y)\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics=\"accuracy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "model.fit(x_train, y_train, epochs=20, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet5 + fashion MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "# BatchNormalization : Layer사이에 중간 결과 데이터들을 표준정규화하는 계층\n",
    "# 중간 레이어가 들어가면 가중치가 붙어서 결과가 튈 수 있다. 정확도를 높이기 위해 한번 값이 계산된 후 정규화를 해준다.\n",
    "# Convolution 후 activation 하기전 normalization 하는 것이 좋다.\n",
    "X = tf.keras.Input(shape=[28,28,1])\n",
    "\n",
    "H = tf.keras.layers.Conv2D(6,5,padding='same')(X)\n",
    "H = tf.keras.layers.BatchNormalization()(H)\n",
    "H = tf.keras.layers.Activation(\"swish\")(H)\n",
    "H = tf.keras.layers.MaxPool2D()(H)\n",
    "\n",
    "H = tf.keras.layers.Conv2D(16,5)(H)\n",
    "H = tf.keras.layers.BatchNormalization()(H)\n",
    "H = tf.keras.layers.Activation(\"swish\")(H)\n",
    "H = tf.keras.layers.MaxPool2D()(H)\n",
    "\n",
    "H = tf.keras.layers.Flatten()(H)\n",
    "\n",
    "H = tf.keras.layers.Dense(120)(H)\n",
    "H = tf.keras.layers.BatchNormalization()(H)\n",
    "H = tf.keras.layers.Activation(\"swish\")(H)\n",
    "\n",
    "H = tf.keras.layers.Dense(84)(H)\n",
    "H = tf.keras.layers.BatchNormalization()(H)\n",
    "H = tf.keras.layers.Activation(\"swish\")(H)\n",
    "\n",
    "Y = tf.keras.layers.Dense(10, activation='softmax')(H)\n",
    "\n",
    "model = tf.keras.Model(X,Y)\n",
    "model.compile(loss='categorical_crossentropy', metrics='accuracy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "model.fit(x_train, y_train, epochs=20, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.4448 - accuracy: 0.8948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4448123574256897, 0.8948000073432922]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 평가 loss: 0.0777 - accuracy: 0.9703 - val_loss: 0.4561 - val_accuracy: 0.8917\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet5 + cifar10 + dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000, 10)\n",
      "(10000, 32, 32, 3) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# 데이터 준비\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# normalization\n",
    "x_train = x_train/ 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# 원핫인코딩\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_12 (InputLayer)       [(None, 32, 32, 3)]       0         \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 28, 28, 6)         456       \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 28, 28, 6)        24        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 28, 28, 6)         0         \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 14, 14, 6)         906       \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 14, 14, 6)        24        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_20 (Activation)  (None, 14, 14, 6)         0         \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 10, 10, 16)        2416      \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 10, 10, 16)       64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_21 (Activation)  (None, 10, 10, 16)        0         \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 5, 5, 16)          6416      \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 5, 5, 16)         64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_22 (Activation)  (None, 5, 5, 16)          0         \n",
      "                                                                 \n",
      " flatten_6 (Flatten)         (None, 400)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 400)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 120)               48120     \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_23 (Activation)  (None, 120)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 84)                10164     \n",
      "                                                                 \n",
      " batch_normalization_26 (Bat  (None, 84)               336       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_24 (Activation)  (None, 84)                0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 70,320\n",
      "Trainable params: 69,824\n",
      "Non-trainable params: 496\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "# Dorpout : 지정된 확률의 개수만큼 노드를 랜덤하게 제외하고 학습을 진행 \n",
    "# -> overfitting 방지, 성능향상, 앙상블 효과\n",
    "# -> 데이터가 충분하다면 최대한 높게 줄수록 좋다(0.4~0.6)\n",
    "X = tf.keras.Input(shape=[32, 32, 3])\n",
    "\n",
    "H = tf.keras.layers.Conv2D(6, 5)(X)\n",
    "H = tf.keras.layers.BatchNormalization()(H)\n",
    "H = tf.keras.layers.Activation(\"swish\")(H)\n",
    "# H = tf.keras.layers.MaxPool2D()(H)\n",
    "H = tf.keras.layers.Conv2D(6, 5, strides=2, padding=\"same\")(H)\n",
    "H = tf.keras.layers.BatchNormalization()(H)\n",
    "H = tf.keras.layers.Activation(\"swish\")(H)\n",
    "\n",
    "H = tf.keras.layers.Conv2D(16, 5)(H)\n",
    "H = tf.keras.layers.BatchNormalization()(H)\n",
    "H = tf.keras.layers.Activation(\"swish\")(H)\n",
    "# H = tf.keras.layers.MaxPool2D()(H)\n",
    "H = tf.keras.layers.Conv2D(16, 5, strides=2, padding=\"same\")(H)\n",
    "H = tf.keras.layers.BatchNormalization()(H)\n",
    "H = tf.keras.layers.Activation(\"swish\")(H)\n",
    "\n",
    "H = tf.keras.layers.Flatten()(H)\n",
    "\n",
    "H = tf.keras.layers.Dropout(0.6)(H)\n",
    "H = tf.keras.layers.Dense(120)(H)\n",
    "H = tf.keras.layers.BatchNormalization()(H)\n",
    "H = tf.keras.layers.Activation(\"swish\")(H)\n",
    "\n",
    "H = tf.keras.layers.Dense(84)(H)\n",
    "H = tf.keras.layers.BatchNormalization()(H)\n",
    "H = tf.keras.layers.Activation(\"swish\")(H)\n",
    "\n",
    "Y = tf.keras.layers.Dense(10, activation=\"softmax\")(H)\n",
    "\n",
    "model = tf.keras.Model(X, Y)\n",
    "model.compile(loss=\"categorical_crossentropy\", metrics=\"accuracy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "# EarlyStopping : 오버 피팅이 일어나면 멈추고 싶다! 가장 그럴듯한 모델을 빼서 쓰고 싶다.\n",
    "es = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "model.fit(x_train, y_train, epochs=100000, batch_size=128, validation_split=0.1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation\n",
    "- 256x256 ,이미지, 10,000장을 학습용 이미지로 제공\n",
    "\n",
    "- 만장이라는 데이터는 작다.\n",
    "만장을 뻥튀기 하자.\n",
    "\n",
    "- 256x256 px을 224x224로 잘라서 학습으로 사용. 한 픽셀씩 옮겨서 자르면 아까와 다른 이미지. 왜냐면 컴퓨터는 숫자로 이해하기 때문에."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageDataGenerator : 기존 이미지를 변형해서 새로운 이미지를 생성해주는 도구\n",
    "# -> overfitting 방지\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1,\n",
    "    horizontal_flip = False,\n",
    "    rotation_range = 0.1\n",
    ")\n",
    "\n",
    "train_ds = datagen.flow(x_train[:40000], y_train[:40000], batch_size=128)\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((x_train[40000:], y_train[40000:])).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 32, 32, 3) (128, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x,y = next(iter(train_ds))\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "# 먼저 이미지로 학습한 다음, Augmentation을 걸어서 학습을 시킴. \n",
    "# 처음부터 Augmentation으로 하면 시간이 너무 많이 걸린다.\n",
    "\n",
    "# model.fit(train_ds, validation_data=valid_ds, epochs=10)\n",
    "\n",
    "es = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "model.fit(x_train, y_train, epochs=100000, batch_size=128, validation_split=0.1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가\n",
    "#  loss: 0.8446 - accuracy: 0.7041 [0.8445699214935303, 0.7041000127792358]\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Models - VGGNET\n",
    "- 3x3 convolution, stride 1 활용\n",
    "- 간단한 구조로 좋은 성과, 파라미터 개수가 많음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# y_train = tf.keras.utils.to_categorical(y_train)\n",
    "# y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_block(n_filters, n_layers, net):\n",
    "    for i in range(n_layers):\n",
    "        net = tf.keras.layers.Conv2D(n_filters, 3, padding=\"same\")(net)\n",
    "        net = tf.keras.layers.BatchNormalization()(net)\n",
    "        net = tf.keras.layers.Activation('swish')(net)\n",
    "    # net = tf.keras.layers.MaxPool2D()(net)\n",
    "    net = tf.keras.layers.Conv2D(n_filters, 3, strides=2, padding=\"same\")(net)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "X = tf.keras.Input(shape=[32, 32, 3])\n",
    "\n",
    "H = vgg_block(64, 2, X)\n",
    "H = vgg_block(128, 2, H)\n",
    "H = vgg_block(256, 2, H)\n",
    "H = vgg_block(512, 2, H)\n",
    "H = vgg_block(512, 2, H)\n",
    "\n",
    "H = tf.keras.layers.Flatten()(H)\n",
    "H = tf.keras.layers.Dense(4096)(H)\n",
    "H = tf.keras.layers.BatchNormalization()(H)\n",
    "H = tf.keras.layers.Activation('swish')(H)\n",
    "H = tf.keras.layers.Dense(4096)(H)\n",
    "H = tf.keras.layers.BatchNormalization()(H)\n",
    "H = tf.keras.layers.Activation('swish')(H)\n",
    "H = tf.keras.layers.Dense(1000)(H)\n",
    "H = tf.keras.layers.BatchNormalization()(H)\n",
    "H = tf.keras.layers.Activation('swish')(H)\n",
    "\n",
    "Y = tf.keras.layers.Dense(10, activation='softmax')(H)\n",
    "\n",
    "model = tf.keras.Model(X, Y)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", metrics=\"accuracy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "es = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "model.fit(x_train, y_train, epochs=1000000, validation_split=0.1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 생성\n",
    "X = tf.keras.Input(shape=[32, 32, 3])\n",
    "\n",
    "H = vgg_block(64, 2, X)\n",
    "H = vgg_block(128, 2, H)\n",
    "H = vgg_block(256, 4, H)\n",
    "H = tf.keras.layers.Dropout(0.5)(H)\n",
    "H = vgg_block(512, 4, H)\n",
    "H = vgg_block(512, 4, H)\n",
    "H = tf.keras.layers.Dropout(0.5)(H)\n",
    "\n",
    "H = tf.keras.layers.Flatten()(H)\n",
    "H = tf.keras.layers.Dense(4096)(H)\n",
    "H = tf.keras.layers.BatchNormalization()(H)\n",
    "H = tf.keras.layers.Activation('swish')(H)\n",
    "H = tf.keras.layers.Dropout(0.6)(H)\n",
    "H = tf.keras.layers.Dense(4096)(H)\n",
    "H = tf.keras.layers.BatchNormalization()(H)\n",
    "H = tf.keras.layers.Activation('swish')(H)\n",
    "H = tf.keras.layers.Dropout(0.6)(H)\n",
    "H = tf.keras.layers.Dense(1000)(H)\n",
    "H = tf.keras.layers.BatchNormalization()(H)\n",
    "H = tf.keras.layers.Activation('swish')(H)\n",
    "\n",
    "Y = tf.keras.layers.Dense(10, activation='softmax')(H)\n",
    "\n",
    "model = tf.keras.Model(X, Y)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", metrics=\"accuracy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "es = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "model.fit(x_train, y_train, epochs=1000000, validation_split=0.1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=False,\n",
    "    rotation_range=0.1,\n",
    ")\n",
    "train_ds = datagen.flow(x_train[:40000], y_train[:40000], batch_size=128)\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((x_train[40000:], y_train[40000:])).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "es = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "result = model.fit(train_ds, validation_data=valid_ds, epochs=100000, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가 loss: 0.6141 - accuracy: 0.8112 [0.6141185164451599, 0.8112000226974487]\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(result.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# plt.plot(result.history['loss'])\n",
    "# plt.plot(result.history['val_loss'])\n",
    "plt.plot([10, 5, 3, 2, 1])\n",
    "plt.plot([7, 5, 4, 3, 4])\n",
    "plt.legend(['loss', 'val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model.save(\"vgg_model.h5\", include_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 모델 로드\n",
    "vgg = tf.keras.models.load_model('vgg_model.h5')\n",
    "vgg.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Models - ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block_2(n_filter, x):\n",
    "    h = tf.keras.layers.Conv2D(n_filter, 3, padding=\"same\")(x)\n",
    "    h = tf.keras.layers.BatchNormalization()(h)\n",
    "    h = tf.keras.layers.Activation(\"swish\")(h)\n",
    "\n",
    "    h = tf.keras.layers.Conv2D(n_filter, 3, padding=\"same\")(h)\n",
    "    h = tf.keras.layers.BatchNormalization()(h)\n",
    "\n",
    "    # x.shape (8, 8, 64)\n",
    "    if x.shape[-1] != n_filter:\n",
    "        x = tf.keras.layers.Conv2D(n_filter, 1, padding=\"same\")(x)\n",
    "\n",
    "    h = tf.keras.layers.Add()([x, h])\n",
    "    h = tf.keras.layers.Activation(\"swish\")(h)\n",
    "    return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.keras.Input(shape=[32, 32, 3])\n",
    "H = tf.keras.layers.Conv2D(64, 5, padding=\"same\")(X)\n",
    "H = tf.keras.layers.BatchNormalization()(H)\n",
    "H = tf.keras.layers.Activation(\"swish\")(H)\n",
    "H = tf.keras.layers.MaxPool2D()(H)\n",
    "\n",
    "for i in range(3):\n",
    "    H = residual_block_2(64, H)\n",
    "H = tf.keras.layers.MaxPool2D()(H)\n",
    "\n",
    "for i in range(4):\n",
    "    H = residual_block_2(128, H)\n",
    "H = tf.keras.layers.MaxPool2D()(H)\n",
    "\n",
    "for i in range(6):\n",
    "    H = residual_block_2(256, H)\n",
    "H = tf.keras.layers.MaxPool2D()(H)\n",
    "\n",
    "for i in range(3):\n",
    "    H = residual_block_2(512, H)\n",
    "H = tf.keras.layers.AvgPool2D()(H)\n",
    "\n",
    "H = tf.keras.layers.Flatten()(H)\n",
    "H = tf.keras.layers.Dense(1000)(H)\n",
    "H = tf.keras.layers.BatchNormalization()(H)\n",
    "H = tf.keras.layers.Activation(\"swish\")(H)\n",
    "Y = tf.keras.layers.Dense(10, activation=\"softmax\")(H)\n",
    "\n",
    "model = tf.keras.Model(X, Y)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", metrics=\"accuracy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 \n",
    "es = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "model.fit(x_train, y_train, epochs=1000000, validation_split=0.1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가  loss: 0.8578 - accuracy: 0.7177 [0.8577784299850464, 0.7177000045776367]\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet 152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block_3(n_filter, x):\n",
    "    h = tf.keras.layers.Conv2D(n_filter // 4, 1, padding=\"same\")(x)\n",
    "    h = tf.keras.layers.BatchNormalization()(h)\n",
    "    h = tf.keras.layers.Activation(\"swish\")(h)\n",
    "\n",
    "    h = tf.keras.layers.Conv2D(n_filter // 4, 3, padding=\"same\")(h)\n",
    "    h = tf.keras.layers.BatchNormalization()(h)\n",
    "    h = tf.keras.layers.Activation(\"swish\")(h)\n",
    "\n",
    "    h = tf.keras.layers.Conv2D(n_filter, 1, padding=\"same\")(h)\n",
    "    h = tf.keras.layers.BatchNormalization()(h)\n",
    "\n",
    "    # x.shape (8, 8, 64)\n",
    "    if x.shape[-1] != n_filter:\n",
    "        x = tf.keras.layers.Conv2D(n_filter, 1, padding=\"same\")(x)\n",
    "\n",
    "    h = tf.keras.layers.Add()([x, h])\n",
    "    h = tf.keras.layers.Activation(\"swish\")(h)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.keras.Input(shape=[32, 32, 3])\n",
    "H = tf.keras.layers.Conv2D(64, 5, padding=\"same\")(X)\n",
    "H = tf.keras.layers.BatchNormalization()(H)\n",
    "H = tf.keras.layers.Activation(\"swish\")(H)\n",
    "# H = tf.keras.layers.MaxPool2D()(H)\n",
    "H = tf.keras.layers.Conv2D(64, 3, strides=2, padding=\"same\")(H)\n",
    "H = tf.keras.layers.Dropout(0.6)(H)\n",
    "\n",
    "for i in range(3):\n",
    "    H = residual_block_3(256, H)\n",
    "# H = tf.keras.layers.MaxPool2D()(H)\n",
    "H = tf.keras.layers.Conv2D(256, 3, strides=2, padding=\"same\")(H)\n",
    "\n",
    "for i in range(8):\n",
    "    H = residual_block_3(512, H)\n",
    "# H = tf.keras.layers.MaxPool2D()(H)\n",
    "H = tf.keras.layers.Conv2D(512, 3, strides=2, padding=\"same\")(H)\n",
    "H = tf.keras.layers.Dropout(0.6)(H)\n",
    "\n",
    "for i in range(36):\n",
    "    H = residual_block_3(1024, H)\n",
    "# H = tf.keras.layers.MaxPool2D()(H)\n",
    "H = tf.keras.layers.Conv2D(1024, 3, strides=2, padding=\"same\")(H)\n",
    "H = tf.keras.layers.Dropout(0.6)(H)\n",
    "\n",
    "for i in range(3):\n",
    "    H = residual_block_3(2048, H)\n",
    "# H = tf.keras.layers.AvgPool2D()(H)\n",
    "H = tf.keras.layers.Conv2D(2048, 3, strides=2, padding=\"same\")(H)\n",
    "\n",
    "H = tf.keras.layers.Dropout(0.6)(H)\n",
    "H = tf.keras.layers.Flatten()(H)\n",
    "H = tf.keras.layers.Dense(1000)(H)\n",
    "H = tf.keras.layers.BatchNormalization()(H)\n",
    "H = tf.keras.layers.Activation(\"swish\")(H)\n",
    "Y = tf.keras.layers.Dense(10, activation=\"softmax\")(H)\n",
    "\n",
    "model = tf.keras.Model(X, Y)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", metrics=\"accuracy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "es = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "model.fit(x_train, y_train, epochs=1000000, batch_size=128, validation_split=0.1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    horizontal_flip=False,\n",
    "    rotation_range=0.1,\n",
    ")\n",
    "train_ds = datagen.flow(x_train[:40000], y_train[:40000], batch_size=128)\n",
    "valid_ds = tf.data.Dataset.from_tensor_slices((x_train[40000:], y_train[40000:])).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "es = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "model.fit(train_ds, validation_data=valid_ds, epochs=100000, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가 loss: 0.5395 - accuracy: 0.8311 [0.5395180583000183, 0.8310999870300293]\n",
    "model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81d63421dc4ff2febe5ae8ebd642fed2f46fe94c0192df0a316b0a71a90664ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
